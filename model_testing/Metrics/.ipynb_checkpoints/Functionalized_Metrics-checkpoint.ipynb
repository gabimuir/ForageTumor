{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a work in progress-let me know if there are any problems. I am currently investigating a potential bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_confusion_matrix(predictions, actuals, threshold):\n",
    "    \"\"\"\n",
    "    Returns a confusion matrix whose rows correspond to predicted outputs and\n",
    "        whose columns correspond to actual outputs. \n",
    "        \n",
    "    predictions: 2D numpy array, the output from model.predict.\n",
    "    actuals: 1D numpy array, the test targets--usually test_generator[0][1].\n",
    "    threshold: float, value above which predictions are classified as positive,\n",
    "        below which predictions are classified as negative. \n",
    "    \"\"\"\n",
    "    predictions = predictions.T[0]\n",
    "    predictions = np.ones(len(predictions)) - predictions # This line is needed for \"No Finding\", but may or may not be needed otherwise\n",
    "    bool_predictions = (predictions > threshold)\n",
    "    bool_actuals = (actuals > threshold)\n",
    "    \n",
    "    return confusion_matrix(bool_predictions, bool_actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(predictions, actuals, threshold):\n",
    "    \"\"\"\n",
    "    Prints all metrics associated with the model output. \n",
    "    \n",
    "    predictions: 2D numpy array, the output from model.predict.\n",
    "    actuals: 1D numpy array, the test targets--usually test_generator[0][1].\n",
    "    threshold: float, value above which predictions are classified as positive,\n",
    "        below which predictions are classified as negative.\n",
    "    \"\"\" \n",
    "    TP, FP, FN, TN = get_confusion_matrix(predictions, actuals, threshold).ravel()\n",
    "    \n",
    "    sensitivity = TP / (TP + FN) # This is also recall\n",
    "    specificity = TN / (TN + FP)\n",
    "    precision = TP / (TP +FP)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP +FN)\n",
    "    F1 = 2 / ((1 / precision) + (1 / sensitivity))\n",
    "    \n",
    "    print(\"Sensitivity/recall:  {}\".format(sensitivity))\n",
    "    print(\"Specificity:         {}\".format(specificity))\n",
    "    print(\"Precision:           {}\".format(precision))\n",
    "    print(\"Accuracy:            {}\".format(accuracy))\n",
    "    print(\"F1 score:            {}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def get_roc_curve(predictions, actuals):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve and associated AUC of the model output. \n",
    "    \n",
    "    predictions: 2D numpy array: the output from model.predict.\n",
    "    actuals: 1D numpy array: the test targets--usually test_generator[0][1].\n",
    "    \"\"\"\n",
    "    predictions = predictions.T[0]\n",
    "    FPR, TPR, thresholds = roc_curve(actuals, predictions)\n",
    "    AUC = roc_auc_score(actuals, predictions)\n",
    "    \n",
    "    plt.plot(FPR, TPR, \"b-\")\n",
    "    plt.plot([0, 1], [0, 1], \"r--\")\n",
    "    plt.title(\"Receiver Operating Characteristic \\n AUC: {}\".format(AUC))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.ylabel(\"Sensitivity\")\n",
    "    plt.xlabel(\"1 - Specificity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def get_precision_recall_curve(predictions, actuals):\n",
    "    \"\"\"\n",
    "    Plots the precision-recall curve and associated AUC of the model output. \n",
    "    \n",
    "    predictions: 2D numpy array: the output from model.predict.\n",
    "    actuals: 1D numpy array: the test targets--usually test_generator[0][1].\n",
    "    \"\"\"\n",
    "    predictions = predictions.T[0]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(actuals, predictions)\n",
    "    AUC = auc(recalls, precisions)\n",
    "    \n",
    "    plt.plot(recalls, precisions, \"b-\")\n",
    "    plt.plot([0, 1], [0.5, 0.5], \"r--\")\n",
    "    plt.title(\"Precision-Recall Curve \\n AUC: {}\".format(AUC))\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.ylabel(\"Recall\")\n",
    "    plt.xlabel(\"Precision\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
