{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Some Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten, Concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = ['No Finding',\n",
    "            'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "            'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "            'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "            'Support Devices']\n",
    "\n",
    "PATH = '/Volumes/GBackup/Data/CheXpertDataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_urls(url):\n",
    "    ''' a function to edit the urls to the correct path\n",
    "    '''\n",
    "    pieces = url.split('/')\n",
    "    newurl = ''\n",
    "\n",
    "      #removing the previous path 'CheXpert-v1.0-small' and including the PATH\n",
    "    for i, piece in enumerate(pieces):\n",
    "        if i > 0:\n",
    "            newurl+= '/' + piece\n",
    "        elif i == 0:\n",
    "            newurl += PATH\n",
    "\n",
    "    return newurl\n",
    "\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    Edits the urls, fills the 'maybes' with yes and the nulls with no,\n",
    "    removes outlier ages (age 0 patients), removes 'unknown gender' patients,\n",
    "    collects which type of image it is\n",
    "    '''\n",
    "    df['Path'] = df['Path'].apply(edit_urls)\n",
    "    df[outcomes] = df[outcomes].fillna(0)\n",
    "    df[outcomes] = df[outcomes].replace(-1,1)\n",
    "    df = df[df['Age'] > 1]\n",
    "    df = df[(df['Sex'] == 'Male') | (df['Sex'] == 'Female')]\n",
    "    df['Image Type'] = df['AP/PA'].fillna('Lateral')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Frontal/Lateral</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>Image Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>87</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>83</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>Lateral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lateral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Volumes/GBackup/Data/CheXpertDataset/train/pa...</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>Frontal</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path     Sex  Age  \\\n",
       "0  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   68   \n",
       "1  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   87   \n",
       "2  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   83   \n",
       "3  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   83   \n",
       "4  /Volumes/GBackup/Data/CheXpertDataset/train/pa...    Male   41   \n",
       "5  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   20   \n",
       "6  /Volumes/GBackup/Data/CheXpertDataset/train/pa...  Female   20   \n",
       "7  /Volumes/GBackup/Data/CheXpertDataset/train/pa...    Male   33   \n",
       "8  /Volumes/GBackup/Data/CheXpertDataset/train/pa...    Male   33   \n",
       "9  /Volumes/GBackup/Data/CheXpertDataset/train/pa...    Male   33   \n",
       "\n",
       "  Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         Frontal    AP         1.0                         0.0           0.0   \n",
       "1         Frontal    AP         0.0                         0.0           1.0   \n",
       "2         Frontal    AP         0.0                         0.0           0.0   \n",
       "3         Lateral   NaN         0.0                         0.0           0.0   \n",
       "4         Frontal    AP         0.0                         0.0           0.0   \n",
       "5         Frontal    PA         1.0                         0.0           0.0   \n",
       "6         Lateral   NaN         1.0                         0.0           0.0   \n",
       "7         Frontal    PA         1.0                         0.0           0.0   \n",
       "8         Lateral   NaN         1.0                         0.0           0.0   \n",
       "9         Frontal    AP         0.0                         0.0           0.0   \n",
       "\n",
       "   Lung Opacity  Lung Lesion  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "1           1.0          0.0    1.0            1.0        0.0          1.0   \n",
       "2           1.0          0.0    0.0            1.0        0.0          0.0   \n",
       "3           1.0          0.0    0.0            1.0        0.0          0.0   \n",
       "4           0.0          0.0    1.0            0.0        0.0          0.0   \n",
       "5           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "6           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "7           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "8           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "9           0.0          0.0    0.0            0.0        0.0          0.0   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \\\n",
       "0           0.0               0.0            0.0       0.0              1.0   \n",
       "1           0.0               1.0            0.0       1.0              0.0   \n",
       "2           0.0               0.0            0.0       1.0              0.0   \n",
       "3           0.0               0.0            0.0       1.0              0.0   \n",
       "4           0.0               0.0            0.0       0.0              0.0   \n",
       "5           0.0               0.0            0.0       0.0              0.0   \n",
       "6           0.0               0.0            0.0       0.0              0.0   \n",
       "7           0.0               0.0            0.0       0.0              1.0   \n",
       "8           0.0               0.0            0.0       0.0              1.0   \n",
       "9           1.0               0.0            0.0       0.0              0.0   \n",
       "\n",
       "  Image Type  \n",
       "0         AP  \n",
       "1         AP  \n",
       "2         AP  \n",
       "3    Lateral  \n",
       "4         AP  \n",
       "5         PA  \n",
       "6    Lateral  \n",
       "7         PA  \n",
       "8    Lateral  \n",
       "9         AP  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PATH + '/train.csv')\n",
    "df = clean_data(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunch_nonimage_features(df, \n",
    "                            colheaders, \n",
    "                            batch_size):\n",
    "    '''\n",
    "    Creates a batch with (hopefully) the same batches as the \n",
    "    train_generator, to hold nonimage features from the dataframe\n",
    "\n",
    "    The desired shape will be:\n",
    "    array - shape numbatches\n",
    "    array[0] - the number of columns of data\n",
    "    array[0][0] - data for the first column (one piece of data for each member of the batch)\n",
    "    so the shape would be:\n",
    "    (numbatches, numfeatures, batch_size)\n",
    "\n",
    "    Because this is a numpy array, all features must be numeric.\n",
    "    '''\n",
    "\n",
    "    for feat in colheaders:\n",
    "        assert df[feat].dtype in ['int', 'int8', 'int64'], f'Features must be numeric. {feat} is type {df[feat].dtype}'\n",
    "\n",
    "    #use ceiling because we need extras\n",
    "    numbatches = math.ceil(len(df) / batch_size)\n",
    "    numfeatures = len(colheaders)\n",
    "\n",
    "    #making an empty array to fill in with the information of the batches\n",
    "    feats_generator = np.zeros( (numbatches, numfeatures, batch_size) )\n",
    "\n",
    "    #using a for loop but I wonder if there's a way to do it better?\n",
    "    for i in range(numbatches):\n",
    "        startindex = i * batch_size\n",
    "\n",
    "        #if the data doesn't fit evenly into the bunches, manually insert it\n",
    "        if startindex + batch_size > len(df):\n",
    "            numleftover = startindex + batch_size - len(df) - 1\n",
    "            for counter, feat in enumerate(colheaders):\n",
    "                for j in range(0, numleftover):\n",
    "                    feats_generator[i][counter][j] = df[feat].iloc[startindex + j]\n",
    "\n",
    "        else:\n",
    "            stopindex = startindex + batch_size\n",
    "            for counter, feat in enumerate(colheaders):\n",
    "                feats_generator[i][counter] = df[feat].iloc[startindex:stopindex].values\n",
    "\n",
    "    print(f'Collected {numfeatures} feature(s) for {len(df)} images.')\n",
    "    return feats_generator\n",
    "\n",
    "\n",
    "def create_generators(df, datagen, path_col_header, feat_col_headers, target_header, batch_size, \n",
    "                      image_size= (256,256), color_mode = 'grayscale', class_mode = 'binary', SEED=None ):\n",
    "    '''\n",
    "    Takes in the things that we use for making a flow_from_dataframe generator, plus makes a feature array\n",
    "    for non-image features.\n",
    "    '''\n",
    "    if SEED == None:\n",
    "        #generate a random seed that will be constant\n",
    "        SEED = np.random.randint(0,100)\n",
    "\n",
    "    #sampling all of it gets a shuffled dataframe\n",
    "    df_shuffled = df.sample(frac=1, random_state = SEED)\n",
    "\n",
    "    #we need the length to perfectly fit the batch size so we're cutting off the extras \n",
    "    #sorry if this is bad practice\n",
    "    numbatches = math.ceil(len(df_shuffled) // batch_size)\n",
    "    corrected_length = numbatches * batch_size\n",
    "    df_shuffled = df_shuffled.iloc[0:corrected_length]\n",
    "\n",
    "    print(f'{len(df) - corrected_length} samples were removed.')\n",
    "\n",
    "    #get the features before we change the type to string\n",
    "    feats_gen = bunch_nonimage_features(df_shuffled, feat_col_headers, batch_size)\n",
    "    \n",
    "    #then change the types to strings because the flow_from_dataframe needs them to be strings\n",
    "    if type(target_header) == list:\n",
    "        for targ in target_header:\n",
    "            df_shuffled[targ] = df_shuffled[targ].apply(str)\n",
    "    else:\n",
    "        df_shuffled[target_header] = df_shuffled[target_header].apply(str)\n",
    "\n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        df_shuffled, \n",
    "        x_col = path_col_header, \n",
    "        y_col = target_header, \n",
    "        target_size= image_size, \n",
    "        color_mode= color_mode, \n",
    "        class_mode= class_mode, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        seed=SEED, \n",
    "    )\n",
    "\n",
    "    print( f'{numbatches} batches created' )\n",
    "\n",
    "    return (train_generator, feats_gen)\n",
    "\n",
    "def make_iterator(image_generator, feature_generator):\n",
    "    '''\n",
    "    Used to properly use the image and feature generators with the custom non-image feature models\n",
    "    '''\n",
    "    #first make a feature generator for my feature array\n",
    "    feat_gen = (\n",
    "        featgen[i]\n",
    "        for i in range(len(featgen))\n",
    "    )\n",
    "    while True:\n",
    "            imgnext = image_generator.next()\n",
    "            feats = next(feat_gen)\n",
    "            yield [ imgnext[0], feats[0] ], imgnext[1]  #Yield the image, the features, and the labels (from features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_nonimage_features(im_shape, feats_shape):\n",
    "    '''\n",
    "    This model takes in the image data and the feature data.\n",
    "    '''\n",
    "    #create the inputs\n",
    "    img_input = Input(shape=im_shape, name='images')\n",
    "    feat_input = Input(shape=feats_shape, name='xtrafeatures')\n",
    "\n",
    "    #create the layers\n",
    "    #the image layers\n",
    "    x = Conv2D(32, kernel_size=3, activation='relu', input_shape=im_shape) (img_input)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    #the other features layer\n",
    "    y = Dense(4, activation='relu')( feat_input )\n",
    "\n",
    "    #add the image and the features together and have another layer on top\n",
    "    added = Concatenate(axis=-1)([x, y])\n",
    "    # z = Dense(8, activation='relu') (added)\n",
    "\n",
    "    #the final layer\n",
    "    predictions = Dense(2, activation=tf.nn.softmax)(added)\n",
    "\n",
    "    #create the model and return it\n",
    "    return Model(inputs=[img_input, feat_input], outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabimuir/anaconda2/envs/django/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/gabimuir/anaconda2/envs/django/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#split into a training and testing set\n",
    "train_df, val_df = train_test_split( df, test_size=0.2)\n",
    "\n",
    "#convert non-image features to numeric features :)\n",
    "train_df['Sex'] = train_df['Sex'].astype('category').cat.codes\n",
    "train_df['Image Type'] = train_df['Image Type'].astype('category').cat.codes\n",
    "\n",
    "#actually make a smaller sample set to test\n",
    "train_sample = train_df.sample(frac=0.2, random_state = 12)\n",
    "\n",
    "IMG_SIZE = (256,256,1)\n",
    "nonimg_feats = ['Age', 'Sex', 'Image Type']\n",
    "num_nonimg_feats = len(nonimg_feats)\n",
    "\n",
    "model = model_with_nonimage_features( IMG_SIZE, (1,) )\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.train.AdamOptimizer(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 samples were removed.\n",
      "Collected 3 feature(s) for 35744 images.\n",
      "Found 35744 validated image filenames belonging to 2 classes.\n",
      "1117 batches created\n"
     ]
    }
   ],
   "source": [
    "#load the data for the model\n",
    "imgen, featgen = create_generators(\n",
    "    train_sample,\n",
    "    datagen = ImageDataGenerator(rescale=1./255),\n",
    "    path_col_header = 'Path',\n",
    "    feat_col_headers = nonimg_feats,\n",
    "    target_header = 'No Finding',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "images (InputLayer)             (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 254, 254, 32) 320         images[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "xtrafeatures (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2064512)      0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 4)            8           xtrafeatures[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 2064516)      0           flatten_10[0][0]                 \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 2)            4129034     concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 4,129,362\n",
      "Trainable params: 4,129,362\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator only works once i think maybe? I don't understand how this works really. but it will crash if steps_per_epoch * epochs > num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "124/124 [==============================] - 193s 2s/step - loss: 1.5851 - acc: 0.9015\n",
      "Epoch 2/5\n",
      "124/124 [==============================] - 206s 2s/step - loss: 1.6248 - acc: 0.8992\n",
      "Epoch 3/5\n",
      "124/124 [==============================] - 249s 2s/step - loss: 1.6045 - acc: 0.9005\n",
      "Epoch 4/5\n",
      "124/124 [==============================] - 301s 2s/step - loss: 1.5273 - acc: 0.9052\n",
      "Epoch 5/5\n",
      "124/124 [==============================] - 288s 2s/step - loss: 1.6654 - acc: 0.8967\n"
     ]
    }
   ],
   "source": [
    "myiterator = make_iterator(imgen, featgen)\n",
    "history = model.fit_generator(\n",
    "        make_iterator(imgen, featgen),\n",
    "        steps_per_epoch=124,\n",
    "        epochs=5,\n",
    "        validation_data=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234\n",
      "0 samples were removed.\n",
      "Collected 3 feature(s) for 2234 images.\n",
      "Found 2234 validated image filenames belonging to 2 classes.\n",
      "1 batches created\n"
     ]
    }
   ],
   "source": [
    "val_sample = val_df.sample(frac=0.05)\n",
    "print(len(val_sample))\n",
    "\n",
    "#convert non-image features to numeric features :)\n",
    "val_sample['Sex'] = val_sample['Sex'].astype('category').cat.codes\n",
    "val_sample['Image Type'] = val_sample['Image Type'].astype('category').cat.codes\n",
    "\n",
    "val_imgen, val_featgen = create_generators(\n",
    "    val_sample,\n",
    "    datagen = ImageDataGenerator(rescale=1./255),\n",
    "    path_col_header = 'Path',\n",
    "    feat_col_headers = nonimg_feats,\n",
    "    target_header = 'No Finding',\n",
    "    batch_size = len(val_sample),\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-28bbd39a4827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions = model.predict(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_imgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_featgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/anaconda2/envs/django/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/django/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/django/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/django/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/django/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(\n",
    "    make_iterator(val_imgen, val_featgen),\n",
    "    steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
